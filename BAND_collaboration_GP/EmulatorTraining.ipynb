{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "from glob import glob\n",
    "import sys\n",
    "import dill\n",
    "import pickle\n",
    "sys.path.insert(0, path.abspath('./'))\n",
    "\n",
    "from src import workdir, parse_model_parameter_file\n",
    "from src.emulator_BAND import EmulatorBAND\n",
    "from src.emulator import Emulator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to create pickle file with training dataset and the second one with the test points\n",
    "\n",
    "Use the 100 posterior points for the second file and the 1000 LHC points for the training of the emulators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/'\n",
    "path_output = './separate_training_posterior_data/'\n",
    "datasets = ['AuAu7.7_dNdy.pkl',\n",
    "            'AuAu7.7_pTvn.pkl',\n",
    "            'AuAu19p6_dNdy.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta.pkl',\n",
    "            'AuAu19p6_pTvn.pkl',\n",
    "            'AuAu200_dNdy.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta.pkl',\n",
    "            'AuAu200_PHOBOSv2eta.pkl',\n",
    "            'AuAu200_pTvn.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "def check_file_length(filename, expected_length):\n",
    "    with open(f\"{path_output}{filename}\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    if len(data) == expected_length:\n",
    "        print(f\"{filename} has the correct length: {expected_length}\")\n",
    "    else:\n",
    "        print(f\"{filename} does not have the correct length. Expected: {expected_length}, Actual: {len(data)}\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "    \n",
    "    print(f\"The dataset {dataset} has length {len(data)}.\")\n",
    "    # Separate data based on event ID\n",
    "    sorted_event_ids = sorted(data.keys(), key=lambda x: int(x))\n",
    "    first_data = {event_id: data[event_id] for event_id in sorted_event_ids[:1000]}\n",
    "    second_data = {event_id: data[event_id] for event_id in sorted_event_ids[1000:1100]}\n",
    "\n",
    "    # Save separated data to pickle files\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_train.pkl', 'wb') as pf1:\n",
    "        pickle.dump(first_data, pf1)\n",
    "        \n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_posterior.pkl', 'wb') as pf2:\n",
    "        pickle.dump(second_data, pf2)\n",
    "\n",
    "    check_file_length(f'{dataset.split(\".p\")[0]}_train.pkl', 1000)\n",
    "    check_file_length(f'{dataset.split(\".p\")[0]}_posterior.pkl', 100) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the training of the emulators for all datasets\n",
    "\n",
    "After training the emulators, we save them with `dill`, such that they can be reloaded from file for the MCMC later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the training without the `parameterTrafoPCA` (for $\\zeta/s(T)$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_par = '../data/modelDesign_3DMCGlauber.txt'\n",
    "path_input = './separate_training_posterior_data/'\n",
    "path_output = './trained_emulators_no_PCA/'\n",
    "\n",
    "datasets_train = ['AuAu7.7_dNdy_train.pkl',\n",
    "            'AuAu7.7_pTvn_train.pkl',\n",
    "            'AuAu19p6_dNdy_train.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_train.pkl',\n",
    "            'AuAu19p6_pTvn_train.pkl',\n",
    "            'AuAu200_dNdy_train.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_train.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_train.pkl',\n",
    "            'AuAu200_pTvn_train.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "for dataset in datasets_train:\n",
    "    emu1 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCGP', logTrafo=False, parameterTrafoPCA=False)\n",
    "    emu1.trainEmulatorAutoMask()\n",
    "    emu2 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCSK', logTrafo=False, parameterTrafoPCA=False)\n",
    "    emu2.trainEmulatorAutoMask()\n",
    "    emu3 = Emulator(f'{path_input}{dataset}', model_par, npc = 4, logTrafo=False, parameterTrafoPCA=False)\n",
    "    emu3.trainEmulatorAutoMask()\n",
    "\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCGP_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu1, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCSK_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu2, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_Scikit_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu3, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the training with the `parameterTrafoPCA` (for $\\zeta/s(T)$, $\\eta/s(\\mu_B)$, $\\langle y_{\\rm loss}\\rangle(y_{\\rm init})$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_par = '../data/modelDesign_3DMCGlauber.txt'\n",
    "path_input = './separate_training_posterior_data/'\n",
    "path_output = './trained_emulators_PCA/'\n",
    "\n",
    "datasets_train = ['AuAu7.7_dNdy_train.pkl',\n",
    "            'AuAu7.7_pTvn_train.pkl',\n",
    "            'AuAu19p6_dNdy_train.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_train.pkl',\n",
    "            'AuAu19p6_pTvn_train.pkl',\n",
    "            'AuAu200_dNdy_train.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_train.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_train.pkl',\n",
    "            'AuAu200_pTvn_train.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "for dataset in datasets_train:\n",
    "    emu1 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCGP', logTrafo=False, parameterTrafoPCA=True)\n",
    "    emu1.trainEmulatorAutoMask()\n",
    "    emu2 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCSK', logTrafo=False, parameterTrafoPCA=True)\n",
    "    emu2.trainEmulatorAutoMask()\n",
    "    emu3 = Emulator(f'{path_input}{dataset}', model_par, npc = 4, logTrafo=False, parameterTrafoPCA=True)\n",
    "    emu3.trainEmulatorAutoMask()\n",
    "\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCGP_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu1, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCSK_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu2, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_Scikit_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu3, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an 'experimental' dataset from one of the posterior points for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './separate_training_posterior_data/'\n",
    "path_output = './separate_training_posterior_data/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy_posterior.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "            'AuAu200_pTvn_posterior.pkl',\n",
    "            'AuAu19p6_dNdy_posterior.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu19p6_pTvn_posterior.pkl',\n",
    "            'AuAu7.7_dNdy_posterior.pkl',\n",
    "            'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "first_event = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[0:1]}\n",
    "    first_event.append(test_data)\n",
    "\n",
    "\n",
    "for event_dict in first_event[1:]:\n",
    "    # Get the 'obs' array for the current event\n",
    "    obs_array_new = event_dict['1000']['obs']\n",
    "    \n",
    "    # Extend the 'obs' array of the first element with the values from the current event\n",
    "    first_event[0]['1000']['obs'] = np.concatenate((first_event[0]['1000']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}example_data_test.pkl', 'wb') as pf1:\n",
    "    pickle.dump(first_event[0], pf1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete parameters 16 and 17 from pkl files (bulk_max_rhob2,bulk_max_rhob4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/'\n",
    "path_output = '../data_new/'\n",
    "datasets = ['AuAu7.7_dNdy.pkl',\n",
    "            'AuAu7.7_pTvn.pkl',\n",
    "            'AuAu19p6_dNdy.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta.pkl',\n",
    "            'AuAu19p6_pTvn.pkl',\n",
    "            'AuAu200_dNdy.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta.pkl',\n",
    "            'AuAu200_PHOBOSv2eta.pkl',\n",
    "            'AuAu200_pTvn.pkl',\n",
    "            'AuAu7.7_logdNdy.pkl',\n",
    "            'AuAu19p6_logdNdy.pkl',\n",
    "            'AuAu19p6_logPHOBOSdNdeta.pkl',\n",
    "            'AuAu200_logdNdy.pkl',\n",
    "            'AuAu200_logPHOBOSdNdeta.pkl'\n",
    "            ]\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "for dataset in datasets:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    print(f\"The dataset {dataset} has length {len(data)}.\")\n",
    "    # Separate data based on event ID\n",
    "    sorted_event_ids = sorted(data.keys(), key=lambda x: int(x))\n",
    "    first_data = {event_id: data[event_id] for event_id in sorted_event_ids}\n",
    "\n",
    "    for point in range(len(sorted_event_ids)):\n",
    "        first_data[f'{sorted_event_ids[point]}']['parameter'] = np.delete(first_data[f'{sorted_event_ids[point]}']['parameter'], [16,17])\n",
    "\n",
    "    # Save new data to pickle files\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}.pkl', 'wb') as pf1:\n",
    "        pickle.dump(first_data, pf1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
