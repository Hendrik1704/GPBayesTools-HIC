{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "import dill\n",
    "import pickle\n",
    "sys.path.insert(0, path.abspath('./'))\n",
    "\n",
    "from src.emulator_BAND import EmulatorBAND\n",
    "from src.emulator import Emulator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define function to create pickle file with training dataset and the second one with the test points\n",
    "\n",
    "Use the last 5 posterior points for the second file and the 1000 LHC points + 95 posterior points for the training of the emulators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/'\n",
    "path_output = './separate_training_posterior_data_1095/'\n",
    "datasets = ['AuAu7.7_dNdy.pkl',\n",
    "            'AuAu7.7_pTvn.pkl',\n",
    "            'AuAu19p6_dNdy.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta.pkl',\n",
    "            'AuAu19p6_pTvn.pkl',\n",
    "            'AuAu200_dNdy.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta.pkl',\n",
    "            'AuAu200_PHOBOSv2eta.pkl',\n",
    "            'AuAu200_pTvn.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "def check_file_length(filename, expected_length):\n",
    "    with open(f\"{path_output}{filename}\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    if len(data) == expected_length:\n",
    "        print(f\"{filename} has the correct length: {expected_length}\")\n",
    "    else:\n",
    "        print(f\"{filename} does not have the correct length. Expected: {expected_length}, Actual: {len(data)}\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "    \n",
    "    print(f\"The dataset {dataset} has length {len(data)}.\")\n",
    "    # Separate data based on event ID\n",
    "    sorted_event_ids = sorted(data.keys(), key=lambda x: int(x))\n",
    "    first_data = {event_id: data[event_id] for event_id in sorted_event_ids[:1095]}\n",
    "    second_data = {event_id: data[event_id] for event_id in sorted_event_ids[1095:1100]}\n",
    "\n",
    "    # Save separated data to pickle files\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_train.pkl', 'wb') as pf1:\n",
    "        pickle.dump(first_data, pf1)\n",
    "        \n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_posterior.pkl', 'wb') as pf2:\n",
    "        pickle.dump(second_data, pf2)\n",
    "\n",
    "    check_file_length(f'{dataset.split(\".p\")[0]}_train.pkl', 1095)\n",
    "    check_file_length(f'{dataset.split(\".p\")[0]}_posterior.pkl', 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the training of the emulators for all datasets\n",
    "\n",
    "After training the emulators, we save them with `dill`, such that they can be reloaded from file for the MCMC later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the training without the `parameterTrafoPCA` (for $\\zeta/s(T)$, $\\eta/s(\\mu_B)$, $\\langle y_{\\rm loss}\\rangle(y_{\\rm init})$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_par = '../data/modelDesign_3DMCGlauber.txt'\n",
    "path_input = './separate_training_posterior_data/'\n",
    "path_output = './trained_emulators_no_PCA/'\n",
    "\n",
    "datasets_train = ['AuAu7.7_dNdy_train.pkl',\n",
    "            'AuAu7.7_pTvn_train.pkl',\n",
    "            'AuAu19p6_dNdy_train.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_train.pkl',\n",
    "            'AuAu19p6_pTvn_train.pkl',\n",
    "            'AuAu200_dNdy_train.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_train.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_train.pkl',\n",
    "            'AuAu200_pTvn_train.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "for dataset in datasets_train:\n",
    "    emu1 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCGP', logTrafo=False, parameterTrafoPCA=False)\n",
    "    emu1.trainEmulatorAutoMask()\n",
    "    emu2 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCSK', logTrafo=False, parameterTrafoPCA=False)\n",
    "    emu2.trainEmulatorAutoMask()\n",
    "    emu3 = Emulator(f'{path_input}{dataset}', model_par, npc = 4, logTrafo=False, parameterTrafoPCA=False)\n",
    "    emu3.trainEmulatorAutoMask()\n",
    "\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCGP_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu1, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCSK_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu2, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_Scikit_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu3, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the training with the `parameterTrafoPCA` (for $\\zeta/s(T)$, $\\eta/s(\\mu_B)$, $\\langle y_{\\rm loss}\\rangle(y_{\\rm init})$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_par = '../data/modelDesign_3DMCGlauber.txt'\n",
    "path_input = './separate_training_posterior_data/'\n",
    "path_output = './trained_emulators_PCA/'\n",
    "\n",
    "datasets_train = ['AuAu7.7_dNdy_train.pkl',\n",
    "            'AuAu7.7_pTvn_train.pkl',\n",
    "            'AuAu19p6_dNdy_train.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_train.pkl',\n",
    "            'AuAu19p6_pTvn_train.pkl',\n",
    "            'AuAu200_dNdy_train.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_train.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_train.pkl',\n",
    "            'AuAu200_pTvn_train.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "for dataset in datasets_train:\n",
    "    emu1 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCGP', logTrafo=False, parameterTrafoPCA=True)\n",
    "    emu1.trainEmulatorAutoMask()\n",
    "    emu2 = EmulatorBAND(f'{path_input}{dataset}', model_par, method='PCSK', logTrafo=False, parameterTrafoPCA=True)\n",
    "    emu2.trainEmulatorAutoMask()\n",
    "    emu3 = Emulator(f'{path_input}{dataset}', model_par, npc = 4, logTrafo=False, parameterTrafoPCA=True)\n",
    "    emu3.trainEmulatorAutoMask()\n",
    "\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCGP_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu1, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_PCSK_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu2, f)\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}_Scikit_trained.sav', 'wb') as f:\n",
    "        dill.dump(emu3, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate an 'experimental' dataset from one of the posterior points for closure testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './separate_training_posterior_data_1095/'\n",
    "path_output = './separate_training_posterior_data_1095/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy_posterior.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "            'AuAu200_pTvn_posterior.pkl',\n",
    "            'AuAu19p6_dNdy_posterior.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu19p6_pTvn_posterior.pkl',\n",
    "            'AuAu7.7_dNdy_posterior.pkl',\n",
    "            'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[4:5]}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "\n",
    "for event_dict in event_data[1:]:\n",
    "    # Get the 'obs' array for the current event\n",
    "    obs_array_new = event_dict['1099']['obs']\n",
    "    \n",
    "    # Extend the 'obs' array of the first element with the values from the current event\n",
    "    event_data[0]['1099']['obs'] = np.concatenate((event_data[0]['1099']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}example_data_test_point1099.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './separate_training_posterior_data_1095/'\n",
    "path_output = './separate_training_posterior_data_1095/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy_posterior.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "            'AuAu200_pTvn_posterior.pkl',\n",
    "            'AuAu19p6_dNdy_posterior.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu19p6_pTvn_posterior.pkl',\n",
    "            'AuAu7.7_dNdy_posterior.pkl',\n",
    "            'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[2:3]}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "\n",
    "for event_dict in event_data[1:]:\n",
    "    # Get the 'obs' array for the current event\n",
    "    obs_array_new = event_dict['1097']['obs']\n",
    "    \n",
    "    # Extend the 'obs' array of the first element with the values from the current event\n",
    "    event_data[0]['1097']['obs'] = np.concatenate((event_data[0]['1097']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}example_data_test_point1097.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the point for the test of the logarithmic training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = './separate_training_posterior_data_1095/'\n",
    "path_output = './separate_training_posterior_data_1095/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy_posterior.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu200_PHOBOSv2eta_posterior.pkl',\n",
    "            'AuAu200_pTvn_posterior.pkl',\n",
    "            'AuAu19p6_dNdy_posterior.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta_posterior.pkl',\n",
    "            'AuAu19p6_pTvn_posterior.pkl',\n",
    "            'AuAu7.7_dNdy_posterior.pkl',\n",
    "            'AuAu7.7_pTvn_posterior.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted(data.keys())[4:5]}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "# modify the datasets with the multiplicities and take the log\n",
    "datasets_to_modify = [0,1,4,5,7]\n",
    "for i in datasets_to_modify:\n",
    "    event_data[i]['1099']['obs'][0,:] = np.log(np.abs(event_data[i]['1099']['obs'][0,:]) + 1e-30)\n",
    "    event_data[i]['1099']['obs'][1,:] = np.abs(event_data[i]['1099']['obs'][1,:]/event_data[i]['1099']['obs'][0,:] + 1e-30)\n",
    "\n",
    "for event_dict in event_data[1:]:\n",
    "    # Get the 'obs' array for the current event\n",
    "    obs_array_new = event_dict['1099']['obs']\n",
    "    \n",
    "    # Extend the 'obs' array of the first element with the values from the current event\n",
    "    event_data[0]['1099']['obs'] = np.concatenate((event_data[0]['1099']['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}example_data_test_point1099_LOG.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate one dataset from all of the training and posterior points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/'\n",
    "path_output = './full_data_one_pkl/'\n",
    "datasets_posterior = [\n",
    "            'AuAu200_dNdy.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta.pkl',\n",
    "            'AuAu200_PHOBOSv2eta.pkl',\n",
    "            'AuAu200_pTvn.pkl',\n",
    "            'AuAu19p6_dNdy.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta.pkl',\n",
    "            'AuAu19p6_pTvn.pkl',\n",
    "            'AuAu7.7_dNdy.pkl',\n",
    "            'AuAu7.7_pTvn.pkl']\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "event_data = []\n",
    "for dataset in datasets_posterior:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    # Get the first event from the posterior dataset\n",
    "    sorted_keys = sorted(data.keys())\n",
    "    test_data = {event_id: data[event_id] for event_id in sorted_keys}\n",
    "    event_data.append(test_data)\n",
    "\n",
    "for dataset in event_data[1:]:\n",
    "    for event in sorted_keys:\n",
    "        # Get the 'obs' array for the current event\n",
    "        obs_array_new = dataset[event]['obs']\n",
    "        \n",
    "        # Extend the 'obs' array of the first dataset with the values from the others\n",
    "        event_data[0][event]['obs'] = np.concatenate((event_data[0][event]['obs'], obs_array_new), axis=1)\n",
    "\n",
    "# Save separated data to pickle files\n",
    "with open(f'{path_output}all_points_all_observables.pkl', 'wb') as pf1:\n",
    "    pickle.dump(event_data[0], pf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete parameters 16 and 17 from pkl files (bulk_max_rhob2,bulk_max_rhob4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = '../data/'\n",
    "path_output = '../data_new/'\n",
    "datasets = ['AuAu7.7_dNdy.pkl',\n",
    "            'AuAu7.7_pTvn.pkl',\n",
    "            'AuAu19p6_dNdy.pkl',\n",
    "            'AuAu19p6_PHOBOSdNdeta.pkl',\n",
    "            'AuAu19p6_pTvn.pkl',\n",
    "            'AuAu200_dNdy.pkl',\n",
    "            'AuAu200_PHOBOSdNdeta.pkl',\n",
    "            'AuAu200_PHOBOSv2eta.pkl',\n",
    "            'AuAu200_pTvn.pkl',\n",
    "            'AuAu7.7_logdNdy.pkl',\n",
    "            'AuAu19p6_logdNdy.pkl',\n",
    "            'AuAu19p6_logPHOBOSdNdeta.pkl',\n",
    "            'AuAu200_logdNdy.pkl',\n",
    "            'AuAu200_logPHOBOSdNdeta.pkl'\n",
    "            ]\n",
    "\n",
    "# Check if the output folder exists, if not, create it\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)\n",
    "\n",
    "for dataset in datasets:\n",
    "    current_path = path_data + dataset\n",
    "    with open(current_path, \"rb\") as pf:\n",
    "        data = pickle.load(pf)\n",
    "\n",
    "    print(f\"The dataset {dataset} has length {len(data)}.\")\n",
    "    # Separate data based on event ID\n",
    "    sorted_event_ids = sorted(data.keys(), key=lambda x: int(x))\n",
    "    first_data = {event_id: data[event_id] for event_id in sorted_event_ids}\n",
    "    print(\"Parameters before =\",len(first_data[f'{sorted_event_ids[0]}']['parameter']))\n",
    "\n",
    "    for point in range(len(sorted_event_ids)):\n",
    "        first_data[f'{sorted_event_ids[point]}']['parameter'] = np.delete(first_data[f'{sorted_event_ids[point]}']['parameter'], [16,17])\n",
    "\n",
    "    print(\"Parameters after =\",len(first_data[f'{sorted_event_ids[0]}']['parameter']))\n",
    "    # Save new data to pickle files\n",
    "    with open(f'{path_output}{dataset.split(\".p\")[0]}.pkl', 'wb') as pf1:\n",
    "        pickle.dump(first_data, pf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
